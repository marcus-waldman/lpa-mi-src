% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/KLmixmvrnorm.R
\name{KLmixmvrnorm}
\alias{KLmixmvrnorm}
\title{Kullback-Leibler divergence between MVN mixtures P & Q (e.g. D_KL(P||Q) )}
\usage{
KLmixmvrnorm(P, Q, n = 5e+05)
}
\arguments{
\item{P}{(list) with the following named elements defining the population:
(1) pi-(numeric vector) with K elements containing component probabilities
(2) mu-(matrix) of size JxK, where J is number of items
(3) S-(array) of size JxJxK with within class covariance matrix.}

\item{Q}{(list) with the same named elements as above, except with the estimates instead of the population values.}

\item{n}{(integer) defining the number of Monte Carlo integration points to use (defaults to 5E5).}
}
\value{
(data.frame) with the estimate (est) and standard error (se) of the K-L approximation.
}
\description{
This function uses Monte Carlo integration to approximate the K-L divergence from (the estimated) distribution Q to (the population) distribution P.
}
\examples{
KLmixmvrnorm(P, Q, n = 1E6)
}
